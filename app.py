import streamlit as st
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import networkx as nx
import chardet
import tempfile
import os
import io
from src.processing import (
    text_cleaner, 
    calculate_word_frequency, 
    calculate_tfidf, 
    perform_lda, 
    perform_ner,
    create_cooccurrence_network,
    generate_wordcloud,
    analyze_sentiment
)

# --- Global Setup ---

def setup_matplotlib_font():
    font_names = ['SimHei', 'Microsoft YaHei', 'PingFang SC', 'WenQuanYi Zen Hei']
    try:
        for font in font_names:
            try:
                matplotlib.font_manager.findfont(font)
                plt.rcParams['font.sans-serif'] = [font]
                break
            except: continue
        plt.rcParams['axes.unicode_minus'] = False
    except Exception as e:
        st.warning(f"æ— æ³•è‡ªåŠ¨è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå›¾å½¢ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚é”™è¯¯: {e}")

setup_matplotlib_font()

# --- Main Application UI ---

def main():
    st.title("æ–‡æœ¬åˆ†æä¸å¯è§†åŒ–å·¥å…·")

    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        with st.expander("æ–‡ä»¶ä¸Šä¼ ", expanded=True):
            uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æœ¬æ–‡ä»¶ (.txt)", type=["txt"], accept_multiple_files=True)
            encoding_option = st.selectbox("æ–‡ä»¶ç¼–ç  (å•ä¸ªæ–‡ä»¶ä¸Šä¼ æ—¶ç”Ÿæ•ˆ)", ('è‡ªåŠ¨æ£€æµ‹', 'UTF-8', 'GBK', 'GB18030', 'BIG5'))

        with st.expander("è¯å…¸é…ç½®"):
            user_dict_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰åˆ†è¯è¯å…¸", type=["txt"])
            st.caption("æ ¼å¼: `è‡ªå®šä¹‰è¯ æƒé‡ è¯æ€§` (åä¸¤é¡¹å¯é€‰), æ¯è¡Œä¸€ä¸ªã€‚")
            user_stopwords_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰åœæ­¢è¯ (å°†è¦†ç›–é»˜è®¤è¯è¡¨)", type=["txt"])
            st.caption("æ ¼å¼: æ¯è¡Œä¸€ä¸ªåœæ­¢è¯ã€‚")
            user_sentiment_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰æƒ…æ„Ÿè¯å…¸", type=["csv"])
            st.caption("æ ¼å¼: CSVæ–‡ä»¶, åŒ…å« `word` å’Œ `Emotion` ä¸¤åˆ—ã€‚ä¾‹å¦‚: `é«˜å…´,10`")

        st.header("ğŸ“Š åˆ†ææ–¹æ³•")
        analysis_type = st.radio("é€‰æ‹©åˆ†æ", ("æƒ…æ„Ÿåˆ†æ", "è¯é¢‘åˆ†æ", "TF-IDFå…³é”®è¯æå–", "LDAä¸»é¢˜å»ºæ¨¡", "å®ä½“è¯†åˆ« (NER)", "å®ä½“å…±ç°ç½‘ç»œ", "è¯äº‘å¯è§†åŒ–"))

    if uploaded_files:
        all_raw_text, success_count, error_files = process_uploaded_files(uploaded_files, encoding_option)
        
        if error_files: st.warning(f"ä»¥ä¸‹æ–‡ä»¶è§£ç å¤±è´¥ï¼Œå·²è¢«è·³è¿‡: {', '.join(error_files)}")
        
        if success_count > 0:
            st.success(f"å·²æˆåŠŸåŠ è½½å¹¶åˆå¹¶ {success_count} ä¸ªæ–‡ä»¶ã€‚")
            with st.expander("åŸå§‹æ–‡æœ¬é¢„è§ˆ (å·²åˆå¹¶)", expanded=False):
                st.text_area("åŸå§‹æ–‡æœ¬", all_raw_text, height=200)

            custom_files = {"dict": user_dict_file, "stopwords": user_stopwords_file, "sentiment": user_sentiment_file}
            paths = handle_custom_files(custom_files)
            
            run_analysis(analysis_type, all_raw_text, paths)
            
            cleanup_temp_files(paths.values())
        else:
            st.error("æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶éƒ½æ— æ³•è§£ç ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œç¼–ç è®¾ç½®ã€‚")
    else:
        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª .txt æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")

# --- Helper Functions ---

def decode_with_fallback(raw_bytes, initial_encoding):
    encodings_to_try = list(dict.fromkeys([initial_encoding, 'utf-8', 'gbk', 'gb18030', 'big5']))
    for encoding in encodings_to_try:
        if not encoding: continue
        try: return raw_bytes.decode(encoding)
        except (UnicodeDecodeError, TypeError): continue
    return None

def process_uploaded_files(files, encoding_option):
    all_text, success_count, error_files = "", 0, []
    is_batch = len(files) > 1
    if is_batch: st.info("æ£€æµ‹åˆ°å¤šä¸ªæ–‡ä»¶ï¼Œå°†ä¸ºæ¯ä¸ªæ–‡ä»¶è‡ªåŠ¨æ£€æµ‹å¹¶å°è¯•å¤šç§ç¼–ç ã€‚")

    for file in files:
        raw_bytes = file.read()
        primary_encoding = get_encoding(raw_bytes, 'è‡ªåŠ¨æ£€æµ‹' if is_batch else encoding_option)
        decoded_text = decode_with_fallback(raw_bytes, primary_encoding)
        if decoded_text is not None:
            all_text += decoded_text + "\n"
            success_count += 1
        else:
            error_files.append(file.name)
    return all_text, success_count, error_files

def get_encoding(raw_bytes, encoding_option):
    if encoding_option == 'è‡ªåŠ¨æ£€æµ‹': return chardet.detect(raw_bytes)['encoding']
    return encoding_option

def handle_custom_files(files):
    paths = {"dict": None, "stopwords": None, "sentiment": None}
    for key, file in files.items():
        if file:
            paths[key] = save_temp_file(file)
            st.sidebar.success(f"å·²åŠ è½½: {file.name}")
    return paths

def save_temp_file(uploaded_file):
    suffix = ".csv" if uploaded_file.type == "text/csv" else ".txt"
    with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix=suffix) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        return tmp_file.name

def cleanup_temp_files(paths):
    for path in paths:
        if path and os.path.exists(path): os.remove(path)

def run_analysis(analysis_type, raw_text, paths):
    analyses = {
        "æƒ…æ„Ÿåˆ†æ": run_sentiment_analysis, "è¯é¢‘åˆ†æ": run_word_frequency_analysis, 
        "TF-IDFå…³é”®è¯æå–": run_tfidf_analysis, "LDAä¸»é¢˜å»ºæ¨¡": run_lda_analysis, 
        "å®ä½“è¯†åˆ« (NER)": run_ner_analysis, "å®ä½“å…±ç°ç½‘ç»œ": run_cooccurrence_network_analysis, 
        "è¯äº‘å¯è§†åŒ–": run_wordcloud_analysis
    }
    target_function = analyses.get(analysis_type)
    if not target_function: st.error(f"æœªçŸ¥çš„åˆ†æç±»å‹: {analysis_type}"); return

    if analysis_type == "æƒ…æ„Ÿåˆ†æ":
        with st.spinner('æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...'):
            cleaned_text = text_cleaner(raw_text, paths["dict"], paths["stopwords"])
        target_function(cleaned_text, paths["sentiment"])
    elif analysis_type not in ["å®ä½“è¯†åˆ« (NER)", "å®ä½“å…±ç°ç½‘ç»œ"]:
        with st.spinner('æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...'):
            cleaned_text = text_cleaner(raw_text, paths["dict"], paths["stopwords"])
        target_function(cleaned_text)
    else:
        target_function(raw_text)

# --- Analysis-Specific UI Functions ---

def run_sentiment_analysis(cleaned_text, custom_sentiment_path):
    st.header("æƒ…æ„Ÿåˆ†æç»“æœ")
    with st.spinner('æ­£åœ¨åˆ†ææƒ…æ„Ÿ...'):
        results, error = analyze_sentiment(cleaned_text, custom_sentiment_path)
    
    if error: st.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {error}"); return
    if not results: st.warning("æ— æ³•è®¡ç®—æƒ…æ„Ÿåˆ†æ•°ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ–‡æœ¬è¿‡çŸ­æˆ–ä¸åŒ…å«æœ‰æ•ˆçš„æƒ…æ„Ÿè¯ã€‚"); return

    st.subheader("æ€»ä½“æƒ…æ„Ÿ")
    color = "green" if results['label'] == "æ­£é¢" else "red" if results['label'] == "è´Ÿé¢" else "blue"
    st.markdown(f"æƒ…æ„Ÿå€¾å‘: <font color='{color}'>**{results['label']}**</font>", unsafe_allow_html=True)
    st.metric("å¹³å‡æƒ…æ„Ÿåˆ†æ•°", f"{results['overall_score']:.4f}")
    
    if results['label'] == 'ä¸­æ€§' and results['word_count'] > 0:
        st.info(f"æ£€æµ‹åˆ° {results['word_count']} ä¸ªæƒ…æ„Ÿè¯ï¼Œä½†æ­£é¢å’Œè´Ÿé¢è¯çš„æƒ…æ„Ÿåˆ†æ•°ç›¸äº’æŠµæ¶ˆï¼Œä½¿æ€»ä½“æƒ…æ„Ÿå€¾å‘ä¸ºä¸­æ€§ã€‚")
    elif results['word_count'] == 0:
        st.warning("æœªåœ¨æ–‡æœ¬ä¸­æ£€æµ‹åˆ°ä»»ä½•å·²çŸ¥çš„æƒ…æ„Ÿè¯ã€‚")

    with st.expander("æŸ¥çœ‹ä¸å¯¼å‡ºè¯¦ç»†ç»“æœ", expanded=True):
        df_pos = pd.DataFrame(results['pos_words'])
        df_neg = pd.DataFrame(results['neg_words'])
        
        df_all = pd.concat([df_pos, df_neg], ignore_index=True)
        st.download_button("ğŸ“¥ å¯¼å‡ºä¸º CSV", df_all.to_csv(index=False).encode('utf-8-sig'), 'sentiment_analysis.csv', 'text/csv')

        st.subheader("æ­£é¢æƒ…æ„Ÿè¯")
        st.dataframe(df_pos)
        st.subheader("è´Ÿé¢æƒ…æ„Ÿè¯")
        st.dataframe(df_neg)

def run_word_frequency_analysis(cleaned_text):
    st.header("è¯é¢‘åˆ†æç»“æœ")
    with st.spinner('æ­£åœ¨è®¡ç®—...'):
        word_freq = calculate_word_frequency(cleaned_text)
        if not word_freq: st.warning("æ–‡æœ¬å†…å®¹ä¸ºç©ºæˆ–è¢«å®Œå…¨è¿‡æ»¤ï¼Œæ— æ³•è®¡ç®—è¯é¢‘ã€‚"); return
        df = pd.DataFrame(word_freq, columns=['è¯è¯­', 'é¢‘ç‡']).sort_values(by='é¢‘ç‡', ascending=False)

    with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
        st.download_button("ğŸ“¥ å¯¼å‡ºä¸º CSV", df.to_csv(index=False).encode('utf-8-sig'), 'word_frequency.csv', 'text/csv')
        top_n = st.slider("æ˜¾ç¤ºTop Nè¯æ•°", 5, min(50, len(df)), 20)
        st.dataframe(df.head(top_n))
        st.bar_chart(df.head(top_n).set_index('è¯è¯­'))

def run_tfidf_analysis(cleaned_text):
    st.header("TF-IDFå…³é”®è¯æå–")
    with st.spinner('æ­£åœ¨è®¡ç®—...'):
        df = calculate_tfidf(cleaned_text)
        if df.empty: st.warning("æ–‡æœ¬å†…å®¹ä¸ºç©ºæˆ–è¢«å®Œå…¨è¿‡æ»¤ï¼Œæ— æ³•è®¡ç®—TF-IDFã€‚"); return

    with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
        st.download_button("ğŸ“¥ å¯¼å‡ºä¸º CSV", df.to_csv(index=False).encode('utf-8-sig'), 'tfidf_keywords.csv', 'text/csv')
        top_n = st.slider("æ˜¾ç¤ºTop Nå…³é”®è¯", 5, min(50, len(df)), 20)
        st.dataframe(df.head(top_n))
        st.bar_chart(df.head(top_n).set_index('è¯è¯­'))

def run_lda_analysis(cleaned_text):
    st.header("LDAä¸»é¢˜å»ºæ¨¡")
    num_topics = st.slider("ä¸»é¢˜æ•°é‡", 2, 10, 5)
    top_words = st.slider("æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯æ•°", 5, 20, 10)

    with st.spinner('æ­£åœ¨æå–ä¸»é¢˜...'):
        topics, error = perform_lda(cleaned_text, num_topics, top_words)

    if error: st.warning(error)
    else:
        df = pd.DataFrame(topics)
        with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
            st.download_button("ğŸ“¥ å¯¼å‡ºä¸º CSV", df.to_csv(index=False).encode('utf-8-sig'), 'lda_topics.csv', 'text/csv')
            st.dataframe(df)

def run_ner_analysis(raw_text):
    st.header("å®ä½“è¯†åˆ« (NER)")
    with st.spinner('æ­£åœ¨è¯†åˆ«å®ä½“...'):
        entities, error = perform_ner(raw_text)

    if error: st.error(error)
    elif not entities: st.warning("æœªèƒ½åœ¨æ–‡æœ¬ä¸­è¯†åˆ«å‡ºä»»ä½•å®ä½“ã€‚")
    else:
        df = pd.DataFrame(entities)
        with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
            col1, col2 = st.columns(2)
            col1.download_button("ğŸ“¥ å¯¼å‡ºä¸º CSV", df.to_csv(index=False).encode('utf-8-sig'), 'ner_results.csv', 'text/csv')
            col2.download_button("ğŸ“¥ å¯¼å‡ºä¸º JSON", df.to_json(orient='records', indent=4, force_ascii=False).encode('utf-8'), 'ner_results.json', 'application/json')
            st.dataframe(df)
            st.bar_chart(df['ç±»å‹'].value_counts())

def run_cooccurrence_network_analysis(raw_text):
    st.header("å®ä½“å…±ç°ç½‘ç»œ")
    with st.spinner('æ­£åœ¨æ„å»ºç½‘ç»œ...'):
        G, error = create_cooccurrence_network(raw_text)

    if error: st.warning(error)
    else:
        with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
            graphml_string = "\n".join(nx.generate_graphml(G))
            st.download_button("ğŸ“¥ å¯¼å‡ºä¸º GraphML", graphml_string, "cooccurrence_network.graphml", "application/xml")
            
            fig, ax = plt.subplots(figsize=(12, 10))
            pos = nx.spring_layout(G, k=0.8, iterations=50)
            nx.draw_networkx(G, pos, with_labels=True, node_size=[G.degree(n) * 100 for n in G.nodes()], width=[G[u][v]['weight'] * 0.5 for u, v in G.edges()], node_color='skyblue', edge_color='gray', alpha=0.8, font_size=10)
            st.pyplot(fig)
            
            edges_df = pd.DataFrame([(u, v, d['weight']) for u, v, d in G.edges(data=True)], columns=['å®ä½“1', 'å®ä½“2', 'å…±ç°æ¬¡æ•°']).sort_values(by='å…±ç°æ¬¡æ•°', ascending=False)
            st.dataframe(edges_df)

def run_wordcloud_analysis(cleaned_text):
    st.header("è¯äº‘å¯è§†åŒ–")
    if not cleaned_text.strip(): st.warning("æ–‡æœ¬å†…å®¹ä¸ºç©ºæˆ–è¢«å®Œå…¨è¿‡æ»¤ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘ã€‚"); return

    with st.spinner('æ­£åœ¨ç”Ÿæˆè¯äº‘...'):
        wc, error = generate_wordcloud(cleaned_text)

    if error: st.error(error)
    else:
        with st.expander("æŸ¥çœ‹ä¸å¯¼å‡º", expanded=True):
            img_buffer = io.BytesIO()
            wc.to_image().save(img_buffer, format='PNG')
            st.download_button("ğŸ“¥ å¯¼å‡ºä¸º PNG", img_buffer.getvalue(), "wordcloud.png", "image/png")
            
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wc, interpolation='bilinear')
            ax.axis('off')
            st.pyplot(fig)

if __name__ == "__main__":
    main()
